{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14480598-d54f-48c6-b63d-ca90847bf7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "import math\n",
    "import cv2\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import Initializer\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.python.keras.utils import layer_utils\n",
    "from tensorflow.python.keras.utils import generic_utils\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430ac7ea-f83b-49b9-ae87-1041eed0f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(annotation_path):\n",
    "    all_data = []\n",
    "    class_count = {}\n",
    "    class_mapping = {}\n",
    "    image_data = {}\n",
    "\n",
    "    # Open the annotation file\n",
    "    with open(annotation_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # Split line by commas\n",
    "            line_split = line.strip().split(',')\n",
    "\n",
    "            # Extract fields\n",
    "            img_file = line_split[0]\n",
    "            xmin = int(line_split[1])\n",
    "            ymin = int(line_split[2])\n",
    "            xmax = int(line_split[3])\n",
    "            ymax = int(line_split[4])\n",
    "            local_class = line_split[5]\n",
    "\n",
    "            # Initialize an entry for each image if it doesn’t already exist\n",
    "            if img_file not in image_data:\n",
    "                image_data[img_file] = {\n",
    "                    'filepath': img_file,\n",
    "                    'bboxes': [],\n",
    "                    'overall_label': 'injury_present'  # Default label\n",
    "                }\n",
    "\n",
    "            # Append bounding box details for each image\n",
    "            image_data[img_file]['bboxes'].append({\n",
    "                'x1': xmin,\n",
    "                'y1': ymin,\n",
    "                'x2': xmax,\n",
    "                'y2': ymax,\n",
    "                'class': local_class\n",
    "            })\n",
    "\n",
    "            # Track class occurrences and mapping\n",
    "            if local_class not in class_count:\n",
    "                class_count[local_class] = 1\n",
    "            else:\n",
    "                class_count[local_class] += 1\n",
    "\n",
    "            if local_class not in class_mapping:\n",
    "                class_mapping[local_class] = len(class_mapping)\n",
    "\n",
    "    # Convert image_data dict to list for all_data\n",
    "    all_data = list(image_data.values())\n",
    "    \n",
    "    return all_data, class_count, class_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d25877-13f2-4090-a672-32cdb189e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, class_count, class_mapping = get_data(r'annotation.txt')\n",
    "val_data, class_count, class_mapping = get_data(r'valid_annotation.txt')\n",
    "test_data, class_count, class_mapping = get_data(r'test_annotation.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e0ebb33-649f-4bef-904a-cf3d88e58c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the augmentation parameters\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,       # Random rotation within 20 degrees\n",
    "    width_shift_range=0.2,   # Horizontal shift up to 20% of the image width\n",
    "    height_shift_range=0.2,  # Vertical shift up to 20% of the image height\n",
    "    shear_range=0.2,         # Shear transformations\n",
    "    zoom_range=0.2,          # Random zoom\n",
    "    horizontal_flip=True,    # Randomly flip images horizontally\n",
    "    fill_mode='nearest'      # Fill in missing pixels after transformations\n",
    ")\n",
    "\n",
    "def data_generator(data, class_mapping, batch_size):\n",
    "    while True:  # Loop forever to generate batches indefinitely\n",
    "        for i in range(0, len(data), batch_size): \n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            for item in data[i:i + batch_size]:\n",
    "                # Load and preprocess each image\n",
    "                img = tf.keras.preprocessing.image.load_img(item['filepath'], target_size=(224, 224))\n",
    "                img = tf.keras.preprocessing.image.img_to_array(img) / 255.0  # Normalize the image\n",
    "\n",
    "                # Apply augmentation\n",
    "                img = datagen.random_transform(img)  # Apply random transformations\n",
    "                \n",
    "                batch_images.append(img)\n",
    "\n",
    "                # Get the hemorrhage type from the first bounding box\n",
    "                local_class = item['bboxes'][0]['class']  # Assuming you take the first bounding box class\n",
    "                label = class_mapping[local_class]  # Get numerical label from mapping\n",
    "                batch_labels.append(label)\n",
    "\n",
    "            # Convert the batch_labels to one-hot encoding for multi-class classification\n",
    "            batch_labels = tf.keras.utils.to_categorical(batch_labels, num_classes=len(class_mapping))\n",
    "\n",
    "            # Yield the batch of images and labels as numpy arrays\n",
    "            yield (np.array(batch_images), np.array(batch_labels))\n",
    "\n",
    "# Example class mapping\n",
    "class_mapping = {\n",
    "    'Intraventricular': 0,\n",
    "    'Subdural': 1,\n",
    "    'Intraparenchymal': 2,\n",
    "    'Subarachnoid': 3,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57be9065-a00c-4f74-9701-b64d7b90fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the path to your annotation file\n",
    "# annotation_path = r'annotation.txt'  # Replace with your actual annotation file path\n",
    "\n",
    "# # Get data from the annotation file\n",
    "# all_data, class_count, class_mapping = get_data(annotation_path)\n",
    "\n",
    "# # Check the structure of the loaded data (optional)\n",
    "# print(all_data)\n",
    "# print(class_count)\n",
    "# print(class_mapping)\n",
    "\n",
    "# # Define the batch size\n",
    "# batch_size = 16\n",
    "\n",
    "# # Create a data generator using all_data\n",
    "# def data_generator(data, batch_size):\n",
    "#     while True:  # Loop forever to generate batches indefinitely\n",
    "#         for i in range(0, len(data), batch_size):\n",
    "#             batch_images = []\n",
    "#             batch_labels = []\n",
    "#             for item in data[i:i + batch_size]:\n",
    "#                 # Load and preprocess each image\n",
    "#                 img = tf.keras.preprocessing.image.load_img(item['filepath'], target_size=(300, 300))\n",
    "#                 img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "#                 batch_images.append(img)\n",
    "\n",
    "#                 # Here you could create labels for your bounding boxes and classification\n",
    "#                 # This example assumes binary classification (injury present/absent)\n",
    "#                 label = [0]  # Replace with actual logic to create labels based on your classes\n",
    "#                 batch_labels.append(label)\n",
    "\n",
    "#             yield (np.array(batch_images), np.array(batch_labels))  # Yield as numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79fd0384-6b82-485d-bf0f-f9a269528a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def detection_and_classification_model(num_classes, num_detection_classes):\n",
    "    # Base model for feature extraction\n",
    "    base_model = tf.keras.applications.VGG16(input_shape=(224, 224, 3), include_top=False, weights='imagenet', classifier_activation='softmax')\n",
    "    \n",
    "    # Detection Head for Faster R-CNN (RPN and ROI Pooling layers)\n",
    "    x = base_model.output\n",
    "    rpn_output = layers.Conv2D(512, (3, 3), activation='relu')(x)\n",
    "    # Add layers specific to detection task here, e.g., ROI Pooling, classification layers for each bounding box\n",
    "    # Detection Output - bounding boxes and localized classifications\n",
    "    detection_output = layers.Dense(num_detection_classes, activation='sigmoid', name='detection_output')(rpn_output)\n",
    "\n",
    "    # Global Classification Head (for whole image classification)\n",
    "    x_classification = layers.GlobalAveragePooling2D()(x) #Dimensionality reduction \n",
    "    x_classification = layers.Dense(1024, activation='relu')(x_classification)\n",
    "    # x_classification = layers.Dense(512, activation='relu')(x_classification)\n",
    "    x_classification = layers.Dropout(0.5)(x_classification)\n",
    "    classification_output = layers.Dense(len(class_mapping), activation='softmax', name='classification_output')(x_classification)\n",
    "\n",
    "    # Model with dual outputs\n",
    "    model = models.Model(inputs=base_model.input, outputs=[classification_output, detection_output])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f857b930-27fa-49ad-a82e-f945c0f7d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Assuming 2 classes for overall classification and 3 types of brain injuries for detection\n",
    "num_classes = 1  # e.g., \"no injury\" or \"injury present\"\n",
    "num_detection_classes = len(class_mapping)  # Number of unique local injury classes\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Track validation loss\n",
    "    patience=3,          # Stop after 5 epochs with no improvement\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Build and compile the model\n",
    "model = detection_and_classification_model(num_classes, num_detection_classes)\n",
    "custom_learning_rate = 0.0005\n",
    "optimizer = Adam(learning_rate=custom_learning_rate)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'classification_output': 'categorical_crossentropy',\n",
    "        'detection_output': 'mse'\n",
    "    },\n",
    "    metrics={\n",
    "        'classification_output': ['accuracy'],\n",
    "        'detection_output': ['mse']\n",
    "    }\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86ef94a-8d73-47cf-945e-e0cf4bd634c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m 62/156\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3:50\u001b[0m 2s/step - classification_output_accuracy: 0.8775 - loss: 4.4862"
     ]
    }
   ],
   "source": [
    "train_gen = data_generator(all_data, class_mapping, batch_size=16)\n",
    "val_gen = data_generator(val_data, class_mapping, batch_size=16)\n",
    "\n",
    "# Training loop\n",
    "# history = model.fit(\n",
    "#     train_gen,\n",
    "#     steps_per_epoch=100,    Adjust based on the size of all_data\n",
    "#     epochs=10,  # Choose the number of epochs\n",
    "# )\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=len(all_data) // 16,\n",
    "    # steps_per_epoch=100,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=len(val_data) // 16,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab852124-d078-4843-8c64-a2ce92c5013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data (assuming `test_gen` is defined similarly to `train_gen`)\n",
    "test_gen = data_generator(test_data, class_mapping, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3c857a-60aa-459e-8407-49aaaf8656f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_classification_accuracy, test_detection_mse = model.evaluate(\n",
    "    test_gen,\n",
    "    steps=100,  # Adjust based on the test dataset size\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Print out results\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Classification Accuracy: {test_classification_accuracy}\")\n",
    "print(f\"Detection Mean Squared Error: {test_detection_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b036739-08d3-4124-9841-8a9315011d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image\n",
    "sample_image = tf.keras.preprocessing.image.load_img(r'test/ID_0027df23b_png_jpg.rf.78a4fefadb07409a5671806d177c2168.jpg', target_size=(300, 300))\n",
    "sample_image = tf.keras.preprocessing.image.img_to_array(sample_image) / 255.0\n",
    "sample_image = np.expand_dims(sample_image, axis=0)  # Add batch dimension\n",
    "\n",
    "# Predict\n",
    "classification_pred, detection_pred = model.predict(sample_image)\n",
    "\n",
    "# Process predictions\n",
    "print(\"Classification Prediction:\", classification_pred)  # Injury presence prediction\n",
    "print(\"Detection Prediction:\", detection_pred)            # Bounding box and injury type predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c044930d-5975-4c1c-8f05-04761b8a791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_idx = np.argmax(classification_pred)\n",
    "predicted_class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ace4d-1366-4a93-9e82-1e6760bb97f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = list(class_mapping.keys())[predicted_class_idx]  # Maps index to class name\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d1f68e-78df-4b40-919f-0be31b49ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"Class Probabilities: {classification_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc5384-f019-439e-8f9f-66e4472f5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 300, 300  # or the target size you defined for your images\n",
    "\n",
    "for i, bbox in enumerate(detection_pred[0][0]):  # Example using first set of predictions\n",
    "    x_center, y_center, width, height = bbox\n",
    "    x_min = int((x_center - width / 2) * img_width)\n",
    "    y_min = int((y_center - height / 2) * img_height)\n",
    "    x_max = int((x_center + width / 2) * img_width)\n",
    "    y_max = int((y_center + height / 2) * img_height)\n",
    "    print(f\"Bounding Box {i}: ({x_min}, {y_min}), ({x_max}, {y_max})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15772945-6c83-4a2d-bb3d-e5106fb4d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Load and prepare the image for visualization\n",
    "img = tf.keras.preprocessing.image.array_to_img(sample_image[0])  # Remove batch dimension\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(img)\n",
    "\n",
    "for bbox in detection_pred[0][0]:  # Adjust if needed\n",
    "    x_center, y_center, width, height = bbox\n",
    "    x_min = (x_center - width / 2) * img_width\n",
    "    y_min = (y_center - height / 2) * img_height\n",
    "    rect = patches.Rectangle((x_min, y_min), width * img_width, height * img_height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f1d51-95a1-4822-a0f8-9bc1a3d83364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
